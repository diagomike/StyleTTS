{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09b35085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import re\n",
    "import sys\n",
    "import yaml\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "import click\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# load packages\n",
    "import random\n",
    "import yaml\n",
    "from munch import Munch\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import librosa\n",
    "\n",
    "from models import *\n",
    "from meldataset import build_dataloader\n",
    "from utils import *\n",
    "from optimizers import build_optimizer\n",
    "import time\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "008b368f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple fix for dataparallel that allows access to class attributes\n",
    "class MyDataParallel(torch.nn.DataParallel):\n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return super().__getattr__(name)\n",
    "        except AttributeError:\n",
    "            return getattr(self.module, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fe53c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logging\n",
    "import logging\n",
    "from logging import StreamHandler\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "handler = StreamHandler()\n",
    "handler.setLevel(logging.DEBUG)\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ffb3e059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where the Config Files are loaded from\n",
    "config = {\n",
    "'log_dir': \"Models/LJSpeech\",\n",
    "'first_stage_path': \"first_stage.pth\",\n",
    "'save_freq': 2,\n",
    "'log_interval': 10,\n",
    "'device': \"cpu\",\n",
    "'multigpu': False,\n",
    "'epochs_1st': 200, # number of epochs for first stage training\n",
    "'epochs_2nd': 100, # number of peochs for second stage training\n",
    "'batch_size': 32,\n",
    "'pretrained_model': \"\",\n",
    "'second_stage_load_pretrained': False, # set to true if the pre-trained model is for 2nd stage\n",
    "'load_only_params': False, # set to true if do not want to load epoch numbers and optimizer parameters\n",
    "\n",
    "'train_data': \"Data/train_list.txt\",\n",
    "'val_data': \"Data/val_list.txt\",\n",
    "\n",
    "'F0_path': \"Utils/JDC/bst.t7\",\n",
    "'ASR_config': \"Utils/ASR/config.yml\",\n",
    "'ASR_path': \"Utils/ASR/epoch_00080.pth\",\n",
    "\n",
    "'preprocess_params':{\n",
    "  'sr': 24000,\n",
    "  'spect_params':{\n",
    "    'n_fft': 2048,\n",
    "    'win_length': 1200,\n",
    "    'hop_length': 300\n",
    "  }},\n",
    "    \n",
    "'model_params':{\n",
    "  'hidden_dim': 512,\n",
    "  'n_token': 178,\n",
    "  'style_dim': 128,\n",
    "  'n_layer': 3,\n",
    "  'dim_in': 64,\n",
    "  'max_conv_dim': 512,\n",
    "  'n_mels': 80,\n",
    "  'dropout': 0.2\n",
    "},\n",
    "'loss_params':{\n",
    "    'lambda_mel': 5., # mel reconstruction loss (1st & 2nd stage)\n",
    "    'lambda_adv': 1., # adversarial loss (1st & 2nd stage)\n",
    "    'lambda_reg': 1., # adversarial regularization loss (1st & 2nd stage)\n",
    "    'lambda_fm': 0.1, # feature matching loss (1st & 2nd stage)\n",
    "    \n",
    "    'lambda_mono': 1., # monotonic alignment loss (1st stage, TMA)\n",
    "    'lambda_s2s': 1., # sequence-to-sequence loss (1st stage, TMA)\n",
    "    'TMA_epoch': 20, # TMA starting epoch (1st stage)\n",
    "\n",
    "    # https://github.com/yl4579/StyleTTS/issues/7\n",
    "    'TMA_CEloss': False, # whether to use cross-entropy (CE) loss for TMA\n",
    "\n",
    "    'lambda_F0': 1., # F0 reconstruction loss (2nd stage)\n",
    "    'lambda_norm': 1., # norm reconstruction loss (2nd stage)\n",
    "    'lambda_dur': 1. # duration loss (2nd stage)\n",
    "},\n",
    "'optimizer_params':{\n",
    "  'lr': 0.0001\n",
    "}\n",
    "}\n",
    "#config_path = 'Configs/config.yml' : Load from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "debf9504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities Used In Training [ Val_loss : given as loss_test/iters_test]\n",
    "def save_model(model,key,optimizer,iters,val_loss,epoch,stage=\"first\"):\n",
    "    # Final Save \n",
    "    print('Saving..')\n",
    "    state = {\n",
    "        'net':  {key: model[key].state_dict() for key in model}, \n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'iters': iters,\n",
    "        'val_loss': val_loss, # loss_test / iters_test,\n",
    "        'epoch': epoch,\n",
    "    }\n",
    "    if (stage == 'first'):\n",
    "        save_path = osp.join(log_dir, config.get('first_stage_path', 'first_stage.pth'))\n",
    "    else:\n",
    "        save_path = osp.join(log_dir, 'epoch_2nd_%05d.pth' % epoch)\n",
    "    torch.save(state, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5971729a",
   "metadata": {},
   "source": [
    "# Main Training Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cab129",
   "metadata": {},
   "source": [
    "## First Stage Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c91ee5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'Configs/config.yml' #: Load from file\n",
    "#config = yaml.safe_load(open(config_path))\n",
    "\n",
    "config = config # load configurations\n",
    "\n",
    "import os.path as osp\n",
    "import yaml\n",
    "# for saving the config file to hard-drive\n",
    "def save_config(config, log_dir):\n",
    "    #config_filename = osp.join(log_dir, 'config.yml')\n",
    "    with open(log_dir, 'w') as f:\n",
    "        yaml.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8dfec9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load Settings from Config\n",
    "log_dir = config['log_dir']\n",
    "if not osp.exists(log_dir): os.makedirs(log_dir, exist_ok=True)\n",
    "# shutil.copy(config_path, osp.join(log_dir, osp.basename(config_path)))\n",
    "save_config(config,osp.join(log_dir, osp.basename(config_path)))\n",
    "writer = SummaryWriter(log_dir + \"/tensorboard\")\n",
    "\n",
    "# write logs\n",
    "file_handler = logging.FileHandler(osp.join(log_dir, 'train.log'))\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "file_handler.setFormatter(logging.Formatter('%(levelname)s:%(asctime)s: %(message)s'))\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "batch_size = config.get('batch_size', 10)\n",
    "device = config.get('device', 'cpu')\n",
    "epochs = config.get('epochs_1st', 200)\n",
    "save_freq = config.get('save_freq', 2)\n",
    "train_path = config.get('train_data', None)\n",
    "val_path = config.get('val_data', None)\n",
    "multigpu = config.get('multigpu', False)\n",
    "log_interval = config.get('log_interval', 10)\n",
    "saving_epoch = config.get('save_freq', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4fb8ac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_list, val_list = get_data_path_list(train_path, val_path)\n",
    "\n",
    "train_dataloader = build_dataloader(train_list,\n",
    "                                    batch_size=batch_size,\n",
    "                                    num_workers=8,\n",
    "                                    dataset_config={},\n",
    "                                    device=device)\n",
    "\n",
    "val_dataloader = build_dataloader(val_list,\n",
    "                                  batch_size=batch_size,\n",
    "                                  validation=True,\n",
    "                                  num_workers=2,\n",
    "                                  device=device,\n",
    "                                  dataset_config={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9187cf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained ASR model\n",
    "ASR_config = config.get('ASR_config', False)\n",
    "ASR_path = config.get('ASR_path', False)\n",
    "text_aligner = load_ASR_models(ASR_path, ASR_config)\n",
    "\n",
    "# load pretrained F0 model\n",
    "F0_path = config.get('F0_path', False)\n",
    "pitch_extractor = load_F0_models(F0_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61a22212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_lr': 0.0001, 'pct_start': 0.0, 'epochs': 200, 'steps_per_epoch': 11}\n",
      "{'max_lr': 0.0001, 'pct_start': 0.0, 'epochs': 200, 'steps_per_epoch': 11}\n",
      "{'max_lr': 0.0001, 'pct_start': 0.0, 'epochs': 200, 'steps_per_epoch': 11}\n",
      "{'max_lr': 0.0001, 'pct_start': 0.0, 'epochs': 200, 'steps_per_epoch': 11}\n",
      "{'max_lr': 0.0001, 'pct_start': 0.0, 'epochs': 200, 'steps_per_epoch': 11}\n",
      "{'max_lr': 0.0001, 'pct_start': 0.0, 'epochs': 200, 'steps_per_epoch': 11}\n",
      "{'max_lr': 0.0001, 'pct_start': 0.0, 'epochs': 200, 'steps_per_epoch': 11}\n"
     ]
    }
   ],
   "source": [
    "#load Model and Optimizer\n",
    "scheduler_params = {\n",
    "    \"max_lr\": float(config['optimizer_params'].get('lr', 1e-4)),\n",
    "    \"pct_start\": float(config['optimizer_params'].get('pct_start', 0.0)),\n",
    "    \"epochs\": epochs,\n",
    "    \"steps_per_epoch\": len(train_dataloader),\n",
    "}\n",
    "\n",
    "model = build_model(Munch(config['model_params']), text_aligner, pitch_extractor)\n",
    "\n",
    "_ = [model[key].to(device) for key in model]\n",
    "\n",
    "optimizer = build_optimizer({key: model[key].parameters() for key in model},\n",
    "                                  scheduler_params_dict= {key: scheduler_params.copy() for key in model})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8d2a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If God Willing I find Good GPU Computer #! Don't Run Now\n",
    "# multi-GPU support\n",
    "if multigpu:\n",
    "    for key in model:\n",
    "        model[key] = MyDataParallel(model[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "009092d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue from Checkpoint [Incase Training Fails]\n",
    "if config.get('pretrained_model', '') != '':\n",
    "    model, optimizer, start_epoch, iters = load_checkpoint(model,  optimizer, config['pretrained_model'],\n",
    "                                load_only_params=config.get('load_only_params', True))\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    iters = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18a6a2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Parameters\n",
    "best_loss = float('inf')  # best test loss\n",
    "loss_train_record = list([])\n",
    "loss_test_record = list([])\n",
    "\n",
    "loss_params = Munch(config['loss_params'])\n",
    "TMA_epoch = loss_params.TMA_epoch\n",
    "TMA_CEloss = loss_params.TMA_CEloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a56fcce",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\diago\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"C:\\Users\\diago\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\diago\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\Public\\RTVC\\StyleTTS\\meldataset.py\", line 105, in __getitem__\n    wave, text_tensor, speaker_id = self._load_tensor(data)\n  File \"C:\\Users\\Public\\RTVC\\StyleTTS\\meldataset.py\", line 118, in _load_tensor\n    wave, sr = sf.read(wave_path)\n  File \"C:\\Users\\diago\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\soundfile.py\", line 256, in read\n    with SoundFile(file, 'r', samplerate, channels,\n  File \"C:\\Users\\diago\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\soundfile.py\", line 629, in __init__\n    self._file = self._open(file, mode_int, closefd)\n  File \"C:\\Users\\diago\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\soundfile.py\", line 1183, in _open\n    _error_check(_snd.sf_error(file_ptr),\n  File \"C:\\Users\\diago\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\soundfile.py\", line 1357, in _error_check\n    raise RuntimeError(prefix + _ffi.string(err_str).decode('utf-8', 'replace'))\nRuntimeError: Error opening 'LJSpeech-1.1/wavs/12_d512021.wav': System error.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mL1Loss() \n\u001b[0;32m      7\u001b[0m _ \u001b[38;5;241m=\u001b[39m [model[key]\u001b[38;5;241m.\u001b[39mtrain() \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m model]\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[0;32m     11\u001b[0m     batch \u001b[38;5;241m=\u001b[39m [b\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[0;32m     12\u001b[0m     texts, input_lengths, mels, mel_input_length \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[1;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[0;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[1;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[0;32m    642\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[0;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m--> 644\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\diago\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"C:\\Users\\diago\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\diago\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\Public\\RTVC\\StyleTTS\\meldataset.py\", line 105, in __getitem__\n    wave, text_tensor, speaker_id = self._load_tensor(data)\n  File \"C:\\Users\\Public\\RTVC\\StyleTTS\\meldataset.py\", line 118, in _load_tensor\n    wave, sr = sf.read(wave_path)\n  File \"C:\\Users\\diago\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\soundfile.py\", line 256, in read\n    with SoundFile(file, 'r', samplerate, channels,\n  File \"C:\\Users\\diago\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\soundfile.py\", line 629, in __init__\n    self._file = self._open(file, mode_int, closefd)\n  File \"C:\\Users\\diago\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\soundfile.py\", line 1183, in _open\n    _error_check(_snd.sf_error(file_ptr),\n  File \"C:\\Users\\diago\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\soundfile.py\", line 1357, in _error_check\n    raise RuntimeError(prefix + _ffi.string(err_str).decode('utf-8', 'replace'))\nRuntimeError: Error opening 'LJSpeech-1.1/wavs/12_d512021.wav': System error.\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    running_loss = 0\n",
    "    start_time = time.time()\n",
    "    criterion = nn.L1Loss() \n",
    "\n",
    "    _ = [model[key].train() for key in model]\n",
    "\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "\n",
    "        batch = [b.to(device) for b in batch]\n",
    "        texts, input_lengths, mels, mel_input_length = batch\n",
    "\n",
    "        mask = length_to_mask(mel_input_length // (2 ** model.text_aligner.n_down)).to('cuda')\n",
    "        m = length_to_mask(input_lengths)\n",
    "\n",
    "        text_mask = length_to_mask(input_lengths).to(texts.device)\n",
    "        ppgs, s2s_pred, s2s_attn_feat = model.text_aligner(mels, mask, texts)\n",
    "\n",
    "        s2s_attn_feat = s2s_attn_feat.transpose(-1, -2)\n",
    "        s2s_attn_feat = s2s_attn_feat[..., 1:]\n",
    "        s2s_attn_feat = s2s_attn_feat.transpose(-1, -2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            text_mask = length_to_mask(input_lengths).to(texts.device)\n",
    "            attn_mask = (~mask).unsqueeze(-1).expand(mask.shape[0], mask.shape[1], text_mask.shape[-1]).float().transpose(-1, -2)\n",
    "            attn_mask = attn_mask.float() * (~text_mask).unsqueeze(-1).expand(text_mask.shape[0], text_mask.shape[1], mask.shape[-1]).float()\n",
    "            attn_mask = (attn_mask < 1)\n",
    "\n",
    "        s2s_attn_feat.masked_fill_(attn_mask, -float(\"inf\"))\n",
    "\n",
    "        if TMA_CEloss:\n",
    "            s2s_attn = F.softmax(s2s_attn_feat, dim=1) # along the mel dimension\n",
    "        else:\n",
    "            s2s_attn = F.softmax(s2s_attn_feat, dim=-1) # along the text dimension\n",
    "\n",
    "        # get monotonic version \n",
    "        with torch.no_grad():\n",
    "            mask_ST = mask_from_lens(s2s_attn, input_lengths, mel_input_length // (2 ** model.text_aligner.n_down))\n",
    "            s2s_attn_mono = maximum_path(s2s_attn, mask_ST)\n",
    "\n",
    "        s2s_attn = torch.nan_to_num(s2s_attn)\n",
    "\n",
    "        # encode\n",
    "        t_en = model.text_encoder(texts, input_lengths, m)\n",
    "\n",
    "        # 50% of chance of using monotonic version\n",
    "        if bool(random.getrandbits(1)):\n",
    "            asr = (t_en @ s2s_attn)\n",
    "        else:\n",
    "            asr = (t_en @ s2s_attn_mono)\n",
    "\n",
    "        # get clips\n",
    "        mel_len = int(mel_input_length.min().item() / 2 - 1)\n",
    "        en = []\n",
    "        gt = []\n",
    "        for bib in range(len(mel_input_length)):\n",
    "            mel_length = int(mel_input_length[bib].item() / 2)\n",
    "\n",
    "            random_start = np.random.randint(0, mel_length - mel_len)\n",
    "            en.append(asr[bib, :, random_start:random_start+mel_len])\n",
    "            gt.append(mels[bib, :, (random_start * 2):((random_start+mel_len) * 2)])\n",
    "\n",
    "        en = torch.stack(en)\n",
    "        gt = torch.stack(gt).detach()\n",
    "\n",
    "        # clip too short to be used by the style encoder\n",
    "        if gt.shape[-1] < 80:\n",
    "            continue\n",
    "\n",
    "        real_norm = log_norm(gt.unsqueeze(1)).squeeze(1).detach()\n",
    "        F0_real, _, _ = model.pitch_extractor(gt.unsqueeze(1))\n",
    "        s = model.style_encoder(gt.unsqueeze(1))\n",
    "\n",
    "        # reconstruction\n",
    "        mel_rec = model.decoder(en, F0_real, real_norm, s)\n",
    "\n",
    "        # discriminator loss\n",
    "        optimizer.zero_grad()\n",
    "        gt.requires_grad_()\n",
    "        out, _ = model.discriminator(gt.unsqueeze(1))\n",
    "        loss_real = adv_loss(out, 1)\n",
    "        loss_reg = r1_reg(out, gt)\n",
    "        out, _ = model.discriminator(mel_rec.detach().unsqueeze(1))\n",
    "        loss_fake = adv_loss(out, 0)\n",
    "        d_loss = loss_real + loss_fake + loss_reg * loss_params.lambda_reg\n",
    "        d_loss.backward()\n",
    "        optimizer.step('discriminator')\n",
    "\n",
    "        # generator loss\n",
    "        optimizer.zero_grad()\n",
    "        loss_mel = criterion(mel_rec, gt)\n",
    "\n",
    "        if epoch > TMA_epoch: # start TMA training\n",
    "            loss_s2s = 0\n",
    "            for _s2s_pred, _text_input, _text_length in zip(s2s_pred, texts, input_lengths):\n",
    "                loss_s2s += F.cross_entropy(_s2s_pred[:_text_length], _text_input[:_text_length])\n",
    "            loss_s2s /= texts.size(0)\n",
    "\n",
    "            if TMA_CEloss:\n",
    "                # cross entropy loss for monotonic alignment\n",
    "                log_attn = torch.nan_to_num(F.log_softmax(s2s_attn_feat, dim=1)) # along the mel dimension\n",
    "                loss_mono = -(torch.mul(log_attn, s2s_attn_mono).sum(axis=[-1, -2]) / input_lengths).mean()\n",
    "            else:\n",
    "                # L1 loss for monotonic alignment\n",
    "                loss_mono = F.l1_loss(s2s_attn, s2s_attn_mono) * 10\n",
    "        else:\n",
    "            loss_s2s = 0\n",
    "            loss_mono = 0\n",
    "\n",
    "        # adversarial loss\n",
    "        with torch.no_grad():\n",
    "            _, f_real = model.discriminator(gt.unsqueeze(1))\n",
    "        out_rec, f_fake = model.discriminator(mel_rec.unsqueeze(1))\n",
    "        loss_adv = adv_loss(out_rec, 1)\n",
    "        # feature matching loss\n",
    "        loss_fm = 0\n",
    "        for m in range(len(f_real)):\n",
    "            for k in range(len(f_real[m])):\n",
    "                loss_fm += torch.mean(torch.abs(f_real[m][k] - f_fake[m][k])) \n",
    "\n",
    "        g_loss = loss_params.lambda_mel * loss_mel + \\\n",
    "            loss_params.lambda_adv * loss_adv + \\\n",
    "            loss_params.lambda_fm * loss_fm + \\\n",
    "            loss_params.lambda_mono * loss_mono + \\\n",
    "            loss_params.lambda_s2s * loss_s2s\n",
    "\n",
    "        running_loss += loss_mel.item()\n",
    "        g_loss.backward()\n",
    "\n",
    "        optimizer.step('text_encoder')\n",
    "        optimizer.step('style_encoder')\n",
    "        optimizer.step('decoder')\n",
    "\n",
    "        if epoch > TMA_epoch: \n",
    "            optimizer.step('text_aligner')\n",
    "            optimizer.step('pitch_extractor')\n",
    "\n",
    "        iters = iters + 1\n",
    "        if (i+1)%log_interval == 0:\n",
    "            logger.info ('Epoch [%d/%d], Step [%d/%d], Mel Loss: %.5f, Adv Loss: %.5f, Disc Loss: %.5f, Mono Loss: %.5f, S2S Loss: %.5f'\n",
    "                    %(epoch+1, epochs, i+1, len(train_list)//batch_size, running_loss / log_interval, loss_adv.item(), d_loss.item(), loss_mono, loss_s2s))\n",
    "\n",
    "            writer.add_scalar('train/mel_loss', running_loss / log_interval, iters)\n",
    "            writer.add_scalar('train/adv_loss', loss_adv.item(), iters)\n",
    "            writer.add_scalar('train/d_loss', d_loss.item(), iters)\n",
    "            writer.add_scalar('train/mono_loss', loss_mono, iters)\n",
    "            writer.add_scalar('train/s2s_loss', loss_s2s, iters)\n",
    "\n",
    "            running_loss = 0\n",
    "            print('Time elasped:', time.time()-start_time)\n",
    "\n",
    "    loss_test = 0\n",
    "\n",
    "    _ = [model[key].eval() for key in model]\n",
    "\n",
    "    # Evaluation\n",
    "    with torch.no_grad():\n",
    "        iters_test = 0\n",
    "        for batch_idx, batch in enumerate(val_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch = [b.to(device) for b in batch]\n",
    "            texts, input_lengths, mels, mel_input_length = batch\n",
    "\n",
    "            with torch.no_grad():\n",
    "                mask = length_to_mask(mel_input_length // (2 ** model.text_aligner.n_down)).to('cuda')\n",
    "                m = length_to_mask(input_lengths)\n",
    "                ppgs, s2s_pred, s2s_attn_feat = model.text_aligner(mels, mask, texts)\n",
    "\n",
    "                s2s_attn_feat = s2s_attn_feat.transpose(-1, -2)\n",
    "                s2s_attn_feat = s2s_attn_feat[..., 1:]\n",
    "                s2s_attn_feat = s2s_attn_feat.transpose(-1, -2)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    text_mask = length_to_mask(input_lengths).to(texts.device)\n",
    "                    attn_mask = (~mask).unsqueeze(-1).expand(mask.shape[0], mask.shape[1], text_mask.shape[-1]).float().transpose(-1, -2)\n",
    "                    attn_mask = attn_mask.float() * (~text_mask).unsqueeze(-1).expand(text_mask.shape[0], text_mask.shape[1], mask.shape[-1]).float()\n",
    "                    attn_mask = (attn_mask < 1)\n",
    "\n",
    "                s2s_attn_feat.masked_fill_(attn_mask, -float(\"inf\"))\n",
    "\n",
    "                if TMA_CEloss:\n",
    "                    s2s_attn = F.softmax(s2s_attn_feat, dim=1) # along the mel dimension\n",
    "                else:\n",
    "                    s2s_attn = F.softmax(s2s_attn_feat, dim=-1) # along the text dimension\n",
    "\n",
    "                # get monotonic version \n",
    "                with torch.no_grad():\n",
    "                    mask_ST = mask_from_lens(s2s_attn, input_lengths, mel_input_length // (2 ** model.text_aligner.n_down))\n",
    "                    s2s_attn_mono = maximum_path(s2s_attn, mask_ST)\n",
    "\n",
    "                s2s_attn = torch.nan_to_num(s2s_attn)\n",
    "\n",
    "            # encode\n",
    "            t_en = model.text_encoder(texts, input_lengths, m)\n",
    "            asr = (t_en @ s2s_attn_mono)\n",
    "\n",
    "            # get clips\n",
    "            mel_len = int(mel_input_length.min().item() / 2 - 1)\n",
    "            en = []\n",
    "            gt = []\n",
    "            for bib in range(len(mel_input_length)):\n",
    "                mel_length = int(mel_input_length[bib].item() / 2)\n",
    "\n",
    "                random_start = np.random.randint(0, mel_length - mel_len)\n",
    "                en.append(asr[bib, :, random_start:random_start+mel_len])\n",
    "                gt.append(mels[bib, :, (random_start * 2):((random_start+mel_len) * 2)])\n",
    "            en = torch.stack(en)\n",
    "            gt = torch.stack(gt).detach()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                F0_real, _, F0 = model.pitch_extractor(gt.unsqueeze(1))\n",
    "                F0 = F0.reshape(F0.shape[0], F0.shape[1] * 2, F0.shape[2], 1).squeeze()\n",
    "\n",
    "            # reconstruct\n",
    "            s = model.style_encoder(gt.unsqueeze(1))\n",
    "            real_norm = log_norm(gt.unsqueeze(1)).squeeze(1)\n",
    "            mel_rec = model.decoder(en, F0_real, real_norm, s)\n",
    "            mel_rec = mel_rec[..., :gt.shape[-1]]\n",
    "\n",
    "            loss_mel = criterion(mel_rec, gt)\n",
    "\n",
    "            loss_test += loss_mel\n",
    "            iters_test += 1\n",
    "\n",
    "    print('Epochs:', epoch + 1)\n",
    "    logger.info('Validation mel loss: %.3f' % (loss_test / iters_test))\n",
    "    print('\\n\\n\\n')\n",
    "\n",
    "    writer.add_scalar('eval/mel_loss', loss_test / iters_test, epoch + 1)\n",
    "    attn_image = get_image(s2s_attn[0].cpu().numpy().squeeze())\n",
    "    writer.add_figure('eval/attn', attn_image, epoch)\n",
    "    mel_image = get_image(mel_rec[0].cpu().numpy().squeeze())\n",
    "    writer.add_figure('eval/mel_rec', mel_image, epoch)\n",
    "\n",
    "    # Save Per 10 Steps of Epoch Runs\n",
    "    if epoch % saving_epoch == 0:\n",
    "        if (loss_test / iters_test) < best_loss:\n",
    "            best_loss = loss_test / iters_test\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net':  {key: model[key].state_dict() for key in model}, \n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'iters': iters,\n",
    "            'val_loss': loss_test / iters_test,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        save_path = osp.join(log_dir, 'epoch_1st_%05d.pth' % epoch)\n",
    "        torch.save(state, save_path)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43bbb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Save \n",
    "print('Saving..')\n",
    "state = {\n",
    "    'net':  {key: model[key].state_dict() for key in model}, \n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'iters': iters,\n",
    "    'val_loss': loss_test / iters_test,\n",
    "    'epoch': epoch,\n",
    "}\n",
    "save_path = osp.join(log_dir, config.get('first_stage_path', 'first_stage.pth'))\n",
    "torch.save(state, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59959214",
   "metadata": {},
   "source": [
    "## Second Stage Training Code (Can Be Run Independantly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfe47fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import os\n",
    "import os.path as osp\n",
    "import re\n",
    "import sys\n",
    "import yaml\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "import click\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# load packages\n",
    "import random\n",
    "import yaml\n",
    "from munch import Munch\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import librosa\n",
    "\n",
    "from models import *\n",
    "from meldataset import build_dataloader\n",
    "from utils import *\n",
    "from optimizers import build_optimizer\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a013ec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data augmentation\n",
    "class TimeStrech(nn.Module):\n",
    "    def __init__(self, scale):\n",
    "        super(TimeStrech, self).__init__()\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        mel_size = x.size(-1)\n",
    "        \n",
    "        x = F.interpolate(x, scale_factor=(1, self.scale), align_corners=False,\n",
    "                          recompute_scale_factor=True, mode='bilinear').squeeze()\n",
    "        \n",
    "        return x.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23179859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple fix for dataparallel that allows access to class attributes\n",
    "class MyDataParallel(torch.nn.DataParallel):\n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return super().__getattr__(name)\n",
    "        except AttributeError:\n",
    "            return getattr(self.module, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd333602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Keepings\n",
    "import logging\n",
    "from logging import StreamHandler\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "handler = StreamHandler()\n",
    "handler.setLevel(logging.DEBUG)\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99835e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-Run Config CELL on top If %Separate Run Of Second Stage or Else Continue\n",
    "config = config # If It Doesn't Work Re-run the Main Config Setup CELL\n",
    "#config = yaml.safe_load(open(config_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9df59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Settings from Config\n",
    "log_dir = config['log_dir']\n",
    "if not osp.exists(log_dir): os.makedirs(log_dir, exist_ok=True)\n",
    "shutil.copy(config_path, osp.join(log_dir, osp.basename(config_path)))\n",
    "writer = SummaryWriter(log_dir + \"/tensorboard\")\n",
    "\n",
    "# write logs\n",
    "file_handler = logging.FileHandler(osp.join(log_dir, 'train.log'))\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "file_handler.setFormatter(logging.Formatter('%(levelname)s:%(asctime)s: %(message)s'))\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "batch_size = config.get('batch_size', 10)\n",
    "device = config.get('device', 'cpu')\n",
    "epochs = config.get('epochs_2nd', 100)\n",
    "save_freq = config.get('save_freq', 2)\n",
    "train_path = config.get('train_data', None)\n",
    "val_path = config.get('val_data', None)\n",
    "multigpu = config.get('multigpu', False)\n",
    "log_interval = config.get('log_interval', 10)\n",
    "saving_epoch = config.get('save_freq', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0044e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_list, val_list = get_data_path_list(train_path, val_path)\n",
    "\n",
    "train_dataloader = build_dataloader(train_list,\n",
    "                                    batch_size=batch_size,\n",
    "                                    num_workers=8,\n",
    "                                    dataset_config={},\n",
    "                                    device=device)\n",
    "\n",
    "val_dataloader = build_dataloader(val_list,\n",
    "                                  batch_size=batch_size,\n",
    "                                  validation=True,\n",
    "                                  num_workers=2,\n",
    "                                  device=device,\n",
    "                                  dataset_config={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230ee00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained ASR model\n",
    "ASR_config = config.get('ASR_config', False)\n",
    "ASR_path = config.get('ASR_path', False)\n",
    "text_aligner = load_ASR_models(ASR_path, ASR_config)\n",
    "\n",
    "# load pretrained F0 model\n",
    "F0_path = config.get('F0_path', False)\n",
    "pitch_extractor = load_F0_models(F0_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8f6155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Model and Optimizer\n",
    "scheduler_params = {\n",
    "    \"max_lr\": float(config['optimizer_params'].get('lr', 1e-4)),\n",
    "    \"pct_start\": float(config['optimizer_params'].get('pct_start', 0.0)),\n",
    "    \"epochs\": epochs,\n",
    "    \"steps_per_epoch\": len(train_dataloader),\n",
    "}\n",
    "\n",
    "model = build_model(Munch(config['model_params']), text_aligner, pitch_extractor)\n",
    "\n",
    "_ = [model[key].to(device) for key in model]\n",
    "\n",
    "optimizer = build_optimizer({key: model[key].parameters() for key in model},\n",
    "                                  scheduler_params_dict= {key: scheduler_params.copy() for key in model})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156f25a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If God Willing I find Good GPU Computer #! Don't Run Now\n",
    "# multi-GPU support\n",
    "if multigpu:\n",
    "    for key in model:\n",
    "        model[key] = MyDataParallel(model[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b320fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Continue from Checkpoint [Incase Training Fails]\n",
    "if config.get('pretrained_model', '') != '' and config.get('second_stage_load_pretrained', False):\n",
    "    model, optimizer, start_epoch, iters = load_checkpoint(model,  optimizer, config['pretrained_model'],\n",
    "                                load_only_params=config.get('load_only_params', True))\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    iters = 0\n",
    "\n",
    "    if config.get('first_stage_path', '') != '':\n",
    "        first_stage_path = osp.join(log_dir, config.get('first_stage_path', 'first_stage.pth'))\n",
    "        print('Loading the first stage model at %s ...' % first_stage_path)\n",
    "        model, optimizer, start_epoch, iters = load_checkpoint(model, optimizer, first_stage_path,\n",
    "                                    load_only_params=True)\n",
    "    else:\n",
    "        raise ValueError('You need to specify the path to the first stage model.') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c844c9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss Variables\n",
    "best_loss = float('inf')  # best test loss\n",
    "loss_train_record = list([])\n",
    "loss_test_record = list([])\n",
    "\n",
    "loss_params = Munch(config['loss_params'])\n",
    "TMA_epoch = loss_params.TMA_epoch\n",
    "TMA_CEloss = loss_params.TMA_CEloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ea1480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Stage Training\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    start_time = time.time()\n",
    "    criterion = nn.L1Loss() \n",
    "\n",
    "    _ = [model[key].eval() for key in model]\n",
    "\n",
    "    model.predictor.train()\n",
    "    model.discriminator.train()\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "\n",
    "        batch = [b.to(device) for b in batch]\n",
    "        texts, input_lengths, mels, mel_input_length = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mask = length_to_mask(mel_input_length // (2 ** model.text_aligner.n_down)).to('cuda')\n",
    "            mel_mask = length_to_mask(mel_input_length).to('cuda')\n",
    "            text_mask = length_to_mask(input_lengths).to(texts.device)\n",
    "\n",
    "            _, _, s2s_attn_feat = model.text_aligner(mels, mask, texts)\n",
    "\n",
    "            s2s_attn_feat = s2s_attn_feat.transpose(-1, -2)\n",
    "            s2s_attn_feat = s2s_attn_feat[..., 1:]\n",
    "            s2s_attn_feat = s2s_attn_feat.transpose(-1, -2)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                text_mask = length_to_mask(input_lengths).to(texts.device)\n",
    "                attn_mask = (~mask).unsqueeze(-1).expand(mask.shape[0], mask.shape[1], text_mask.shape[-1]).float().transpose(-1, -2)\n",
    "                attn_mask = attn_mask.float() * (~text_mask).unsqueeze(-1).expand(text_mask.shape[0], text_mask.shape[1], mask.shape[-1]).float()\n",
    "                attn_mask = (attn_mask < 1)\n",
    "\n",
    "            if TMA_CEloss:\n",
    "                s2s_attn = F.softmax(s2s_attn_feat, dim=1) # along the mel dimension\n",
    "            else:\n",
    "                s2s_attn = F.softmax(s2s_attn_feat, dim=-1) # along the text dimension\n",
    "\n",
    "            mask_ST = mask_from_lens(s2s_attn, input_lengths, mel_input_length // (2 ** model.text_aligner.n_down))\n",
    "            s2s_attn_mono = maximum_path(s2s_attn, mask_ST)\n",
    "\n",
    "            # encode\n",
    "            m = length_to_mask(input_lengths)\n",
    "\n",
    "            t_en = model.text_encoder(texts, input_lengths, m)\n",
    "            asr = (t_en @ s2s_attn_mono)\n",
    "\n",
    "            d_gt = s2s_attn_mono.sum(axis=-1).detach()\n",
    "\n",
    "            # compute the style of the entire utterance\n",
    "            # this operation cannot be done in batch because of the avgpool layer (may need to work on masked avgpool)\n",
    "            ss = []\n",
    "            for bib in range(len(mel_input_length)):\n",
    "                mel_length = int(mel_input_length[bib].item())\n",
    "                mel = mels[bib, :, :mel_input_length[bib]]\n",
    "                s = model.style_encoder(mel.unsqueeze(0).unsqueeze(1))\n",
    "                ss.append(s)\n",
    "            s = torch.stack(ss).squeeze()\n",
    "\n",
    "        d, _ = model.predictor(t_en, s, \n",
    "                                                input_lengths, \n",
    "                                                s2s_attn_mono, \n",
    "                                                m)\n",
    "        # augmentation\n",
    "        with torch.no_grad():\n",
    "            M = np.random.random()\n",
    "            ts = TimeStrech(1+ (np.random.random()-0.5)*M*0.5)\n",
    "\n",
    "            mels = ts(mels.unsqueeze(1)).squeeze(1)\n",
    "            mels = mels[:, :, :mels.size(-1) // 2 * 2]\n",
    "\n",
    "            mel_input_length = torch.floor(ts.scale * mel_input_length) // 2 * 2 \n",
    "\n",
    "            mask = length_to_mask(mel_input_length // (2 ** model.text_aligner.n_down)).to('cuda')\n",
    "            mel_mask = length_to_mask(mel_input_length).to('cuda')\n",
    "            text_mask = length_to_mask(input_lengths).to(texts.device)\n",
    "\n",
    "            # might have misalignment due to random scaling\n",
    "            try:\n",
    "                _, _, s2s_attn_feat = model.text_aligner(mels, mask, texts)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            s2s_attn_feat = s2s_attn_feat.transpose(-1, -2)\n",
    "            s2s_attn_feat = s2s_attn_feat[..., 1:]\n",
    "            s2s_attn_feat = s2s_attn_feat.transpose(-1, -2)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                text_mask = length_to_mask(input_lengths).to(texts.device)\n",
    "                attn_mask = (~mask).unsqueeze(-1).expand(mask.shape[0], mask.shape[1], text_mask.shape[-1]).float().transpose(-1, -2)\n",
    "                attn_mask = attn_mask.float() * (~text_mask).unsqueeze(-1).expand(text_mask.shape[0], text_mask.shape[1], mask.shape[-1]).float()\n",
    "                attn_mask = (attn_mask < 1)\n",
    "\n",
    "            if TMA_CEloss:\n",
    "                s2s_attn = F.softmax(s2s_attn_feat, dim=1) # along the mel dimension\n",
    "            else:\n",
    "                s2s_attn = F.softmax(s2s_attn_feat, dim=-1) # along the text dimension\n",
    "\n",
    "            mask_ST = mask_from_lens(s2s_attn, input_lengths, mel_input_length // (2 ** model.text_aligner.n_down))\n",
    "            s2s_attn_mono = maximum_path(s2s_attn, mask_ST)\n",
    "\n",
    "            # encode\n",
    "            asr = (t_en @ s2s_attn_mono)\n",
    "\n",
    "        _, p = model.predictor(t_en, s, \n",
    "                                                input_lengths, \n",
    "                                                s2s_attn_mono, \n",
    "                                                m)\n",
    "\n",
    "        # get clips\n",
    "        mel_len = int(mel_input_length.min().item() / 2 - 1)\n",
    "        en = []\n",
    "        gt = []\n",
    "        p_en = []\n",
    "\n",
    "        for bib in range(len(mel_input_length)):\n",
    "            mel_length = int(mel_input_length[bib].item() / 2)\n",
    "\n",
    "            random_start = np.random.randint(0, mel_length - mel_len)\n",
    "            en.append(asr[bib, :, random_start:random_start+mel_len])\n",
    "            p_en.append(p[bib, :, random_start:random_start+mel_len])\n",
    "            gt.append(mels[bib, :, (random_start * 2):((random_start+mel_len) * 2)])\n",
    "\n",
    "        en = torch.stack(en)\n",
    "        p_en = torch.stack(p_en)\n",
    "        gt = torch.stack(gt).detach()\n",
    "\n",
    "        if gt.size(-1) < 80:\n",
    "            continue\n",
    "\n",
    "        with torch.no_grad():\n",
    "            s = model.style_encoder(gt.unsqueeze(1))\n",
    "\n",
    "            F0_real, _, F0 = model.pitch_extractor(gt.unsqueeze(1))\n",
    "            F0 = F0.reshape(F0.shape[0], F0.shape[1] * 2, F0.shape[2], 1).squeeze()\n",
    "\n",
    "            asr_real = model.text_aligner.get_feature(gt)\n",
    "\n",
    "            N_real = log_norm(gt.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "            mel_rec_gt = model.decoder(en, F0_real, N_real, s)\n",
    "\n",
    "        F0_fake, N_fake = model.predictor.F0Ntrain(p_en, s)\n",
    "\n",
    "        mel_rec = model.decoder(en, F0_fake, N_fake, s)\n",
    "\n",
    "        loss_F0_rec =  (F.smooth_l1_loss(F0_real, F0_fake)) / 10\n",
    "        loss_norm_rec = F.smooth_l1_loss(N_real, N_fake)\n",
    "\n",
    "        # discriminator loss\n",
    "        optimizer.zero_grad()\n",
    "        mel_rec_gt.requires_grad_()\n",
    "        out, _ = model.discriminator(mel_rec_gt.unsqueeze(1))\n",
    "        loss_real = adv_loss(out, 1)\n",
    "        loss_reg = r1_reg(out, mel_rec_gt)\n",
    "        out, _ = model.discriminator(mel_rec.detach().unsqueeze(1))\n",
    "        loss_fake = adv_loss(out, 0)\n",
    "        d_loss = loss_real + loss_fake + loss_reg\n",
    "        d_loss.backward()\n",
    "        optimizer.step('discriminator')\n",
    "\n",
    "        # generator loss\n",
    "        optimizer.zero_grad()\n",
    "        loss_mel = criterion(mel_rec, mel_rec_gt)\n",
    "        loss_dur = 0\n",
    "        for _s2s_pred, _text_input, _text_length in zip(d, d_gt, input_lengths):\n",
    "            loss_dur += F.l1_loss(_s2s_pred[1:_text_length-1], \n",
    "                                        _text_input[1:_text_length-1])\n",
    "        loss_dur /= texts.size(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _, f_real = model.discriminator(mel_rec_gt.unsqueeze(1))\n",
    "        out_rec, f_fake = model.discriminator(mel_rec.unsqueeze(1))\n",
    "        loss_adv = adv_loss(out_rec, 1)\n",
    "\n",
    "        # feature matching loss\n",
    "        loss_fm = 0\n",
    "        for m in range(len(f_real)):\n",
    "            for k in range(len(f_real[m])):\n",
    "                loss_fm += torch.mean(torch.abs(f_real[m][k] - f_fake[m][k]))\n",
    "\n",
    "        g_loss = loss_params.lambda_mel * loss_mel + \\\n",
    "                loss_params.lambda_F0 * loss_F0_rec + \\\n",
    "                loss_params.lambda_dur * loss_dur + \\\n",
    "                loss_params.lambda_norm * loss_norm_rec + \\\n",
    "                loss_params.lambda_adv * loss_adv + \\\n",
    "                loss_params.lambda_fm * loss_fm\n",
    "\n",
    "        running_loss += loss_mel.item()\n",
    "        g_loss.backward()\n",
    "        if torch.isnan(g_loss):\n",
    "            from IPython.core.debugger import set_trace\n",
    "            set_trace()\n",
    "        optimizer.step('predictor')\n",
    "\n",
    "        iters = iters + 1\n",
    "        if (i+1)%log_interval == 0:\n",
    "            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.5f, Avd Loss: %.5f,  Disc Loss: %.5f, Dur Loss: %.5f, Norm Loss: %.5f, F0 Loss: %.5f'\n",
    "                    %(epoch+1, epochs, i+1, len(train_list)//batch_size, running_loss / log_interval, loss_adv, d_loss, loss_dur, loss_norm_rec, loss_F0_rec))\n",
    "\n",
    "            writer.add_scalar('train/mel_loss', running_loss / log_interval, iters)\n",
    "            writer.add_scalar('train/adv_loss', loss_adv.item(), iters)\n",
    "            writer.add_scalar('train/d_loss', d_loss.item(), iters)\n",
    "            writer.add_scalar('train/dur_loss', loss_dur, iters)\n",
    "            writer.add_scalar('train/norm_loss', loss_norm_rec, iters)\n",
    "            writer.add_scalar('train/F0_loss', loss_F0_rec, iters)\n",
    "\n",
    "            running_loss = 0\n",
    "            print('Time elasped:', time.time()-start_time)\n",
    "\n",
    "    loss_test = 0\n",
    "    loss_align = 0\n",
    "    _ = [model[key].eval() for key in model]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        iters_test = 0\n",
    "        for batch_idx, batch in enumerate(val_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch = [b.to(device) for b in batch]\n",
    "            texts, input_lengths, mels, mel_input_length = batch\n",
    "            with torch.no_grad():\n",
    "                mask = length_to_mask(mel_input_length // (2 ** model.text_aligner.n_down)).to('cuda')\n",
    "                text_mask = length_to_mask(input_lengths).to(texts.device)\n",
    "\n",
    "                _, _, s2s_attn_feat = model.text_aligner(mels, mask, texts)\n",
    "\n",
    "                s2s_attn_feat = s2s_attn_feat.transpose(-1, -2)\n",
    "                s2s_attn_feat = s2s_attn_feat[..., 1:]\n",
    "                s2s_attn_feat = s2s_attn_feat.transpose(-1, -2)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    text_mask = length_to_mask(input_lengths).to(texts.device)\n",
    "                    attn_mask = (~mask).unsqueeze(-1).expand(mask.shape[0], mask.shape[1], text_mask.shape[-1]).float().transpose(-1, -2)\n",
    "                    attn_mask = attn_mask.float() * (~text_mask).unsqueeze(-1).expand(text_mask.shape[0], text_mask.shape[1], mask.shape[-1]).float()\n",
    "                    attn_mask = (attn_mask < 1)\n",
    "\n",
    "                if TMA_CEloss:\n",
    "                    s2s_attn = F.softmax(s2s_attn_feat, dim=1) # along the mel dimension\n",
    "                else:\n",
    "                    s2s_attn = F.softmax(s2s_attn_feat, dim=-1) # along the text dimension\n",
    "\n",
    "                mask_ST = mask_from_lens(s2s_attn, input_lengths, mel_input_length // (2 ** model.text_aligner.n_down))\n",
    "                s2s_attn_mono = maximum_path(s2s_attn, mask_ST)\n",
    "\n",
    "                # encode\n",
    "                m = length_to_mask(input_lengths)\n",
    "                t_en = model.text_encoder(texts, input_lengths, m)\n",
    "                asr = (t_en @ s2s_attn_mono)\n",
    "\n",
    "                d_gt = s2s_attn_mono.sum(axis=-1).detach()\n",
    "\n",
    "            # compute the style of the entire utterance\n",
    "            # this operation cannot be done in batch because of the avgpool layer (may need to work on masked avgpool)\n",
    "            ss = []\n",
    "            for bib in range(len(mel_input_length)):\n",
    "                mel_length = int(mel_input_length[bib].item())\n",
    "                mel = mels[bib, :, :mel_input_length[bib]]\n",
    "                s = model.style_encoder(mel.unsqueeze(0).unsqueeze(1))\n",
    "                ss.append(s)\n",
    "            s = torch.stack(ss).squeeze()\n",
    "\n",
    "            d, p = model.predictor(t_en, s, \n",
    "                                                input_lengths, \n",
    "                                                s2s_attn_mono, \n",
    "                                                m)\n",
    "            # get clips\n",
    "            mel_len = int(mel_input_length.min().item() / 2 - 1)\n",
    "            en = []\n",
    "            gt = []\n",
    "            p_en = []\n",
    "\n",
    "\n",
    "            for bib in range(len(mel_input_length)):\n",
    "                mel_length = int(mel_input_length[bib].item() / 2)\n",
    "\n",
    "                random_start = np.random.randint(0, mel_length - mel_len)\n",
    "                en.append(asr[bib, :, random_start:random_start+mel_len])\n",
    "                p_en.append(p[bib, :, random_start:random_start+mel_len])\n",
    "\n",
    "                gt.append(mels[bib, :, (random_start * 2):((random_start+mel_len) * 2)])\n",
    "\n",
    "            en = torch.stack(en)\n",
    "            p_en = torch.stack(p_en)\n",
    "            gt = torch.stack(gt).detach()\n",
    "\n",
    "            s = model.style_encoder(gt.unsqueeze(1))\n",
    "\n",
    "            F0_fake, N_fake = model.predictor.F0Ntrain(p_en, s)\n",
    "\n",
    "            loss_dur = 0\n",
    "            for _s2s_pred, _text_input, _text_length in zip(d, d_gt, input_lengths):\n",
    "                loss_dur += F.l1_loss(_s2s_pred[1:_text_length-1], \n",
    "                                            _text_input[1:_text_length-1])\n",
    "            loss_dur /= texts.size(0)\n",
    "\n",
    "            mel_rec = model.decoder(en, F0_fake, N_fake, s)\n",
    "            mel_rec = mel_rec[..., :gt.shape[-1]]\n",
    "\n",
    "            loss_mel = criterion(mel_rec, gt)\n",
    "\n",
    "            loss_test += loss_mel\n",
    "            loss_align += loss_dur\n",
    "            iters_test += 1\n",
    "\n",
    "    print('Epochs:', epoch + 1)\n",
    "    print('Validation loss: %.3f, %.3f' % (loss_test / iters_test, loss_align / iters_test), '\\n\\n\\n')\n",
    "\n",
    "    writer.add_scalar('eval/mel_loss', loss_test / iters_test, epoch + 1)\n",
    "    writer.add_scalar('eval/dur_loss', loss_align / iters_test, epoch + 1)\n",
    "\n",
    "    if epoch % saving_epoch == 0:\n",
    "        if (loss_test / iters_test) < best_loss:\n",
    "            best_loss = loss_test / iters_test\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net':  {key: model[key].state_dict() for key in model}, \n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'iters': iters,\n",
    "            'val_loss': loss_test / iters_test,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        save_path = osp.join(log_dir, 'epoch_2nd_%05d.pth' % epoch)\n",
    "        torch.save(state, save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
